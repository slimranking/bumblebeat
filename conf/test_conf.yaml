data_root: 'data/' # Directory ot store data
batch_size: 6 # Number of batches to split data into
tgt_len: 512 # Number of steps to predict

data:
    data_dir: {{data_root}}
    dataset: 'groove/full-full-midionly' # Datset name (to be downloaded from tensorflow dataset)
    tgt_len: {{tgt_len}} 
    per_host_train_bsz: {{batch_size}}
    per_host_valid_bsz: {{batch_size}}

processing:
    quantize: True # After this line for groove converter
    steps_per_quarter: 4 # Number of quantized steps in a quarter
    filter_4_4: # Whether to filter for only 4/4 time signatures
    max_tensors_per_notesequence: 20

model:
    ## GPU ##
    num_core_per_host: 3 # Number of cores to use, should be a divisor of batch size

    ## Experiment (data/checkpoint/directory) ##
    data_dir: "{{data_root}}/tfrecords" # Where to find tfrecords of training data
    record_info_dir: "{{data_root}}/tfrecords/" # Where to find record info corpus
    corpus_info_path: "{{data_root}}/corpus-info.json" # Where to find corpus info json
    model_dir: 'EXP-"{{dataset}}' # Where to store trained model
    do_train: False # Whether to run training
    do_eval: True # Whether to run eval on the dev set
    eval_ckpt_path:  # Checkpoint path for do_test evaluation. If set, model_dir will be ignored. If unset, will use the latest ckpt in model_dir
    warm_start_path: # Checkpoint path for warm start. If set, will clear Adam states. Note that the new model_dir should be different from warm_start_path
    
    ## Optimization ##
    learning_rate: 0.00025 # Maximum learning rate
    warmup_steps: 0 # Number of steps for linear lr warmup
    clip: 0.25 # Gradient clipping value
    min_lr_ratio: 0.004 # Minimum ratio learning rate (for cosine decay)
    
    ## Training ##
    train_steps: 1 # Total umber of training steps
    train_batch_size: {{batch_size}} # Size of train batch
    eval_batch_size: {{batch_size}} # Size of valid batch
    iterations: 200 # Number of iterations per repeat loop
    save_steps: 4000 # Number of steps for model checkpointing

    ## Evaluation config ##
    do_test: True # Run on the test set
    max_eval_batch: -1 # Set -1 to turn off. Only used in test mode
    do_eval_only: False # Run evaluation only
    start_eval_steps: 10000 # Which checkpoint to start with in `do_eval_only` mode
    eval_split: "test" # Which data split to evaluate

    ## Model ##
    tgt_len: {{tgt_len}} 
    mem_len: 512 # Number of steps to cache
    same_length: False # Same length attention
    clamp_len: 1 # Clamp length
    n_layer: 12 # Number of layers
    d_model: 512 # Dimension of model
    d_embed: 512 # Dimension of embeddings
    n_head: 8 # Number of attention heads
    d_head: 64 # Dimension of each attention head
    d_inner: 2048 # Dimension of inner hidden size in positionwise feed-forward
    dropout: 0 # Dropout rate
    dropatt: 0 # Attention dropout rate
    untie_r: False # untie r_w_bias and r_r_bias?
 
    ## Adaptive Softmax / Embedding ##
    tie_weight: True  # Tie embedding and softmax weight.
    div_val: 1  # Divide the embedding size by this val for each bin
    proj_share_all_but_first: False  # True to share all but first projs, False not to share.
    proj_same_dim: True  # Project the bin with the same dimension.

    ## Parameter Initialization ##
    init: "normal"  # ["normal", "uniform"],
    init_std: 0.02  # Initialization std when init is normal.
    proj_init_std: 0.01  # Initialization std for embedding projection.
    init_range: 0.1  # Initialization std when init is uniform.